---
title: "Assessing Treatment Effects of TFD725 for Non-small Cell Lung Cancer"
output:
  html_document:
    df_print: paged
    fig_caption: yes
    number_sections: yes
  pdf_document: default
---

<style type="text/css">

body{ /* Normal  */
      font-size: 18px;
  }

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE,message=FALSE,warning=FALSE)
knitr::opts_chunk$set(echo = TRUE, message = FALSE,warning = FALSE)
set.seed(1)
library(ggplot2) # ggplot 
library(knitr)  # file knitting
library(magrittr) # %>% 
library(dplyr) # grammar of data manipulation
library(tidyverse) # grammar of data manipulation
library(ggcorrplot) # correlation plot
library(rpart) # decision tree
library(rpart.plot) # decision tree plot
library(ISLR)
library(caret)
library(gbm)
library(ROCR)
library(corrplot)
library(MASS)
library(caTools)
library(rsample)
library(class)
library(kknn) #K-Nearest Neighbor
library(AUC)
library(rmarkdown)
library(e1071)
library(DT)
library(xtable)
library(kableExtra)
library(devtools)
library(pander)
```

***

Team ID:

Name (tasks): 

Name (tasks):

Name (tasks):

Auditor (tasks):

***

# Introduction

## Background


In this project, we analyze empirical data from a 2008 telemarketing campaign from a Portuguese retail bank aimed at subscribing new users to a term deposit. The original data at predicting the likelihood of a client subscribing to a long-term deposit. 
The authors of this dataset wanted to predict whether a client will subscribe to a long-term deposit. More information is available on [this website](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#) and the original paper by Moro, Cortez, and Rita (2014).


The goal of this project is to understand the success of the telemarketing campaign of the Portuguese bank in getting customers to sign on for long-term deposits. We try to find out if certain factors such as their job, marital status, employment status, or salary are factors that affect the decision on signing onto the long term deposits. 
Through the use of data science methods and data wrangling, we would analyze the data by using methods such as linear regression, logistic regression model, Random forest models to analyze the components of each data. Further, classification trees and a PCA could also be used to better visualize the relationship between each categorical variable. 
We will be using the dataset downloaded from the UCI Machine Learning Repository, the dataset is related to the direct marketing campaign of a Portuguese Banking Institution aimed at subscribing new clients to a term deposit. The dataset is available for download at [this link] (https://archive.ics.uci.edu/ml/datasets/bank+marketing) and we will be using the bank-additional-full.csv dataset for this particular data analysis project. The bank-additional-full dataset contains client details such as age, marital status, gender, education, job, etc. These factors will be used in building the predictive model and in data analysis methodologies. 


## Statistical questions of interest



To answer the primary scientific question of interest, we propose to fit a linear regression with the observation time (from randomization) as the response and the indicator of region, age, indicator of gender, indicator of advanced stage, indicator for whether patient had an abnormal LDF level at time of randomization, indicator whether patient had an abnormal alkaline phoshatase level at time of randomization, and patient's performance on ECOG scale as covariates. To answer the secondary questions of interest, we will use the Cox proportional hazard model to estimate the survival rate, and we will add interaction terms between the treatment indicator and the regions of patients, indicator that patient had an abnormal LDF level, or indicator that patient had an abnormal alkaline phoshatase level.

Primary Question:  Build a prediction model for whether a client will sign on to a long-term deposit.

To answer the primary scientific question of interest, we propose to make a Decision Tree, fit a general linear regression with the subscription result as the response and the rest of the variables as the indicators, calculate the K-Nearest Neighbor, and perform Support Vector Machine on the data to create a prediction model to determine whether a client will sign on to a long-term deposit or not. 

Secondary Questions: What “type” of client data from our dataset most strongly predicts if a client will sign onto a long-term deposit? (e.g. Client Background Data, Telemarketing Data, Social-Economic Data, Other)

To answer the seconday question, 

# Analysis Plan


The bank-additional-full data consisted of different client data variables such as, age, marital status, gender, education, job. Ultimately, the purpose of the data was to find out if there is a relation between the different variables and what affects them to signing a long term deposit. This is such that a prediction model can be built to understand and better able know the factors on determining a long term deposit and also if their social-economic status will affect their decisions. In our data analysed, we selected duration, campaign, emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed, job, marital, education, default, housing, loan, contact, month, day of week and outcome, as factors to be analysed. In doing so, we identified through correlation testing for the numeric variables the factors that affect the deposit decisions. 

Using predictive models such as, classification and regression trees to visualise the data such that the target’s variable values could be predicted on other values, K Nearest Neighbour to approximate the association between the independent variables and the continuous outcomes, Confusion matrix for Random Forest, to determine the best model for the data set, Logistic regression model to find the accuracy of the plot which there after allowed us to plot the PCA for the levels of different factors affecting the decision. We also used a Support Vector Matrix (SVM) which was used to predict the decision on the long term deposit based on the factors. 


## Population and study design


The data set is related to the marketing campaign of a Portuguese banking institution. The data collected was from May 2008 to November 2010 based on the marketing campaign of phone calls. The product sold here was the long term deposit which usually needed more than one call to the person to get a decision. The data collected focused on 17 key attributes which are broken into 4 subcategories- Bank client data, Related to the last contact with the campaign, social-economic context attributes, and other attributes. The output of this is whether a client had subscribed to a long term deposit. 

We identified the main key attributes that are significant to the study used. This also includes determining the data type, continuous variable or categorical variable. The study did analysis for the attributes and identified the significance of each variable on the outcome. 

Our 20 attributes can be categorized into 4 types of different client data:
  1) Client Background Data: age, job, marital status, education, default, housing and loan
  2) Bank Telemarketing data: contact, month, day of the week, and duration.
  3) Socio-Economic Data: employment variation rate, consumer price index, consumer confidence index, 3 month Euribor rate, and number of employees.
  4) Other Data: campaign, past days, previous, and past outcome.


We will be focusing on this attribute categorization to answer our secondary question using exploratory analysis.
.

## Statistical Analysis

### Descriptive Analysis
These dataset attributes denote customer data, socio-economic data, telemarketing data, and some other data. Some attributes are numerical, and some are categorical. For data validation, we first check our dataset for duplicate rows, missing data, and missing value by variable. Next for data cleaning, we removed rows with columns that have missing values, saved deduplicated rows, and imputed missing values. Some attributes needed transformation to numeric class for fitting the models. Therefore, we used the as.numeric function. After the final removal of duplicate rows, we verify that our new compact “bank” dataset is fully cleaned. Finally, we coded a binary response (yes/no) to represent our response variable (y). 

#### Set up and Data Wrangling

For this data analysis project, we will be using the bank-additional-full.csv dataset.

```{r, echo=FALSE,warning = FALSE}
setwd("/Users/trucle/Desktop/STA\ 138/Bank/bank-additional")
# for this project we will be using the bank-additional-full.csv to do our analysis on.
```

Change csv data into dataframe.
```{r, echo=TRUE,warning = FALSE}
write.csv(bank_additional_full, "cleaned_bank_additional_full.csv") 
```
Save the newly modified dataset into it's own csv file, in addition, delete an unnecessary row. 
```{r, echo=TRUE}
bank <- read.csv("cleaned_bank_additional_full.csv")[-1] 
```

Transform all of the quantitative values into numeric class for easier manipulation later on. 
```{r, echo=TRUE}
we transformed all of the quantitative values into numeric class for easier manipulation later on. 
bank$age <- as.numeric(bank$age)
bank$duration <- as.numeric(bank$duration)
bank$campaign <- as.numeric(bank$campaign)
bank$pdays <- as.numeric(bank$pdays)
bank$previous <- as.numeric(bank$previous)
bank$emp.var.rate <- as.numeric(bank$emp.var.rate)
bank$cons.price.idx <- as.numeric(bank$cons.price.idx)
bank$cons.conf.idx <- as.numeric(bank$cons.conf.idx)
bank$nr.employed <- as.numeric(bank$nr.employed)
```
Checking if there are any missing data within each categories and push them into a category of there own. 
We also change the subscription category into binary as well. 
```{r, echo=TRUE}
# this checks if there are any missing data within each categories and push them into a category of there own. 
# this shows the counts for each groups within each categories, run summary(bank) to see the result
bank$job = fct_explicit_na(bank$job, "missing")
bank$marital = fct_explicit_na(bank$marital, "missing")
bank$education = fct_explicit_na(bank$education, "missing")
bank$default = fct_explicit_na(bank$default, "missing")
bank$loan = fct_explicit_na(bank$loan, "missing")
bank$contact = fct_explicit_na(bank$contact, "missing")
bank$poutcome = fct_explicit_na(bank$poutcome, "missing")
bank$day_of_week = fct_explicit_na(bank$day_of_week, "missing")
bank$housing = fct_explicit_na(bank$housing, "missing")
bank$month = fct_explicit_na(bank$month, "missing")

bank$y =ifelse(bank$y =='yes',1,0) # transforming 'yes' category into a binary 1=yes 0=no
```

### "Main" Analysis


<span style="color:red">Note: </span>  Describe the analysis you plan to conduct to answer the questions of interest. Remember to rename the section title to fit the actual analysis you propose, e.g., inferential analysis, clustering, prediction model, etc. For instance, in this report, this section should explain

* Detailed model specification of linear regression model.
* Description of the Cox proportional hazard model.
* Description of the model with interactions.

# Mention methods implemented for extra-credit seprately

* Implement a method not discussed in class for one task
* Explain why you chose this method
* You can be asked questions about this in the presentation



# Results

<span style="color:red">Note: </span> In this template, we present the unpolished tables and figures generated with generic `r` functions, and we skip all interpretation of the analysis results.  You can adapt any functions in this section in your report.


## Descriptive Analysis


<span style="color:red">Note: </span> There are many ways to automatically generate summary tables in `R`, of which many would even generate html or latex tables. In this template, we will use the package `qwraps2` as an example ([link](https://cran.r-project.org/web/packages/qwraps2/vignettes/summary-statistics.html)).

```{r }
library(dplyr)
library(tidyr)
library(qwraps2)

options(digits = 3)  

set.seed(1) # Read more about RNGs with ?set.seed

### Read data
nsclc_data <- read.table("nsclc-modified.txt", header = TRUE)

excluded_vars = c("ptid","tx");
table1_summary <-  nsclc_data %>%   select(.,-one_of(excluded_vars)) %>%  qsummary(.)

table1 =nsclc_data %>% group_by(.data$tx) %>%
  summary_table(., table1_summary)

rgroups =   rep(4,length(names(nsclc_data))-2)
names(rgroups)=names(nsclc_data)[!names(nsclc_data)%in%excluded_vars]

qable(table1,markup='markdown',rgroup=rgroups)
```




```{r}
nsclc_data.trt= nsclc_data %>% filter(tx == 1);
nsclc_data.ctrl= nsclc_data %>% filter(tx == 0);
table2_summary.trt <- nsclc_data.trt %>%   select("obstime","survival.past.400") %>%  qsummary(.)
table2_summary.ctrl <- nsclc_data.ctrl %>%   select("obstime","survival.past.400") %>%  qsummary(.)

table2=list();
table2.trt=list();
table2.ctrl=list();
bin_list = c("europe",             "abnLDH","abnAlkphos")
for(i_var in 1:length(bin_list)){
(table2.trt[[i_var]] =nsclc_data.trt %>% group_by(.data[[bin_list[i_var]]]) %>% summary_table(., table2_summary.trt))
(table2.ctrl[[i_var]] =nsclc_data.ctrl %>% group_by(.data[[bin_list[i_var]]]) %>% summary_table(., table2_summary.ctrl))

table2[[i_var]] = cbind(table2.trt[[i_var]],table2.ctrl[[i_var]]);
}
rgroups2 =   rep(4,2)
names(rgroups2)=c("obstime","survival.past.400")
```

<span style="color:red">Note: </span>  Draw the Kaplan-Meier curve with no adjusted variables [Link](https://rviews.rstudio.com/2017/09/25/survival-analysis-with-r/).

```{r}
library(ggplot2)
library(survival)
library(ggfortify)
km_fit <- survfit(Surv(obstime, death) ~ tx, data=nsclc_data)

autoplot(km_fit)
```

```{r}
qable(table2[[1]],markup="markdown",rgroup=rgroups2)
```


```{r}
qable(table2[[2]],markup="markdown",rgroup=rgroups2)
```


```{r}
qable(table2[[3]],markup="markdown",rgroup=rgroups2)
```

## Inferential Analysis

```{r, cache = TRUE, message=FALSE}
library(lmtest)
library(sandwich)

nsclc_data.lm=lm(obstime~tx+europe+age+male+advdis+response+abnLDH+abnAlkphos+as.factor(ECOG),data=nsclc_data);

nsclc_data.robust_lm=coeftest(nsclc_data.lm, vcov = vcovHC(nsclc_data.lm, type="HC1"))

tmp=matrix(nsclc_data.robust_lm,nrow=dim(nsclc_data.robust_lm)[1],ncol=dim(nsclc_data.robust_lm)[2])
dimnames(tmp)=dimnames(nsclc_data.robust_lm);

caption_text = paste('Table 3: Results of multiple linear regression', sep='')
knitr::kable(
 tmp,  caption = caption_text
)
```



```{r,message=FALSE}
### Fit a Cox proportional hazard model

nsclc_data.cox <- coxph(Surv(obstime, death) ~tx+europe+age+male+advdis+response+abnLDH+abnAlkphos+as.factor(ECOG), data = nsclc_data)

sum_cox=summary(nsclc_data.cox)

caption_text = paste('Table 4: Results of Cox proportional hazard model', sep='')
knitr::kable(
 sum_cox$coef,  caption = caption_text
)
```

Check if adding more interactions will change results.

```{r}

nsclc_data.cox_int <- coxph(Surv(obstime, death) ~tx*(europe+age+male+advdis+response+abnLDH+abnAlkphos)+as.factor(ECOG), data = nsclc_data)

sum_cox_int=summary(nsclc_data.cox_int)

caption_text = paste('Table 5: Results of Cox proportional hazard model with interaction terms', sep='')
knitr::kable(
 sum_cox_int$coef,  caption = caption_text
)

```
```{r}
lklh_ratio=anova(nsclc_data.cox_int,nsclc_data.cox)

```

The likelihood ratio test for the two models yields a p-value of `r lklh_ratio[[4]][2]`.

# Mention results for extra-credit method (if any) 

* Mention results for extra-credit method here
# Discussion

<span style="color:red">Note: </span> Discuss the limitations of the presented projects, and comment on how this project enlightens future research or analysis.

# Session information
```{r}
print(sessionInfo(), local = FALSE)
```
